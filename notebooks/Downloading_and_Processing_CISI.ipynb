{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtumEnV8l+MAdD+AjL7iX4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrogengo/CISI_BM25/blob/main/Downloading_and_Processing_CISI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descrição\n",
        "\n",
        "Esse notebook tem por objetivo realizar o download e o processamento do conjunto de dados CISI (Centre for Inventions and Scientific Information). Os dados estão divididos em 4 arquivos:\n",
        "\n",
        "- cisi.all: arquivo de texto que contém 1460 documentos.\n",
        "- cisi.qry: arquivo de texto que contém 112 queries.\n",
        "- cisi.rel: arquivo de texto que contém pares de queries e documentos relevantes.\n",
        "- cisi.bln: arquivo de texto com lista de queries booleanas (não será usado).\n",
        "\n",
        "A parte mais importante na hora de processar esses dados é garantir que as quantidades de dados extraídos batem com as descrições e, além disso, que durante a extração não estamos considerando textos a mais ou a menos."
      ],
      "metadata": {
        "id": "xjM7VP9zroRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Donwload e descompressão dos arquivos"
      ],
      "metadata": {
        "id": "QZifJQuPrnCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4ow5Vx0H2H7",
        "outputId": "4d14ab36-d5ff-47f0-d778-a0784fca98e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-20 02:36:54--  http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
            "Resolving ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)... 130.209.240.253\n",
            "Connecting to ir.dcs.gla.ac.uk (ir.dcs.gla.ac.uk)|130.209.240.253|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 775144 (757K) [application/gzip]\n",
            "Saving to: ‘cisi.tar.gz’\n",
            "\n",
            "cisi.tar.gz         100%[===================>] 756.98K  1.15MB/s    in 0.6s    \n",
            "\n",
            "2023-02-20 02:36:55 (1.15 MB/s) - ‘cisi.tar.gz’ saved [775144/775144]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
        "!tar -xvzf cisi.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Processamento dos dados"
      ],
      "metadata": {
        "id": "kWTQHxHDtPqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CISI.ALL\n",
        "\n",
        "Extrair apenas o texto que aparecer abaixo de `.W`.\n",
        "\n",
        "```\n",
        ".I 6\n",
        ".T\n",
        "Abstracting Concepts and Methods\n",
        ".A\n",
        "Borko, H.\n",
        ".W\n",
        "     Graduate library school study of abstracting should be more than a\n",
        "how-to-do-it course.\n",
        "It should include general material on the characteristics and types of abstracts.\n",
        ".X\n",
        "6 6 6\n",
        "363 1 6\n",
        "403 1 6\n",
        "```\n",
        "\n",
        "### CISI.QRY\n",
        "\n",
        "Extrair apenas o texto que aparecer abaixo de `.W`.\n",
        "\n",
        "```\n",
        ".I 21\n",
        ".W\n",
        "The need to provide personnel for the information field.\n",
        "```\n",
        "\n",
        "### CISI.REL\n",
        "\n",
        "Considerar o par como sendo o primeiro e o segundo número de cada linha. Os últimos (0 e 0.000) podemos desconsiderar.\n",
        "\n",
        "```\n",
        "    21      6 0 0.000000\n",
        "    21     14 0 0.000000\n",
        "    21     22 0 0.000000\n",
        "    21     85 0 0.000000\n",
        "    21    171 0 0.000000\n",
        "    21    185 0 0.000000\n",
        "    21    186 0 0.000000\n",
        "    21    303 0 0.000000\n",
        "    21    339 0 0.000000\n",
        "    21    392 0 0.000000\n",
        "    21    400 0 0.000000\n",
        "```"
      ],
      "metadata": {
        "id": "apBB7LpCtVob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def load_collection(path):\n",
        "    \"\"\"Load the CISI collection from a file.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        collection = f.read()\n",
        "    return collection\n",
        "\n",
        "def parse_documents(collection):\n",
        "    \"\"\"Parse the documents in the CISI collection.\"\"\"\n",
        "    document_pattern = re.compile(r'\\.W\\s+(.*?)\\s+\\.[A-Z]', re.DOTALL)\n",
        "    documents = document_pattern.findall(collection)\n",
        "    documents = [doc.replace(\"\\n\", \" \").strip() for doc in documents]\n",
        "    return documents\n",
        "\n",
        "def parse_queries(path):\n",
        "    \"\"\"Parse the queries in the CISI queries file.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        queries = f.read()\n",
        "    query_pattern = re.compile(r'\\.W\\s*(.*?)\\n+\\.[A-Z]', re.DOTALL)\n",
        "    queries = query_pattern.findall(queries)\n",
        "    queries = [query.replace(\"\\n\", \" \").strip() for query in queries]\n",
        "    return queries\n",
        "\n",
        "def parse_judgments(path):\n",
        "    \"\"\"Parse the relevance judgments in the CISI relevance judgments file.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        judgments = f.read()\n",
        "    judgment_pattern = re.compile(r'\\s+(\\d+)\\s+(\\d+)\\s+', re.DOTALL)\n",
        "    judgments = judgment_pattern.findall(judgments)\n",
        "    judgments_dict = defaultdict(lambda: [])\n",
        "    for query, document in judgments:\n",
        "      judgments_dict[int(query)].append(int(document))\n",
        "    return judgments_dict"
      ],
      "metadata": {
        "id": "1bPPw0riKWD0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = load_collection(\"CISI.ALL\")\n",
        "documents = parse_documents(collection)\n",
        "queries = parse_queries(\"CISI.QRY\")\n",
        "judgments = parse_judgments(\"CISI.REL\")\n",
        "\n",
        "assert len(documents) == 1460\n",
        "assert len(queries) == 112"
      ],
      "metadata": {
        "id": "FMsG2o6_Mtr3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"Tokenize a document or query.\"\"\"\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    return words\n",
        "\n",
        "def build_index(documents):\n",
        "    \"\"\"Build an inverted index from the documents.\"\"\"\n",
        "    index = {}\n",
        "    doc_term_freqs = []\n",
        "\n",
        "    for i, document in enumerate(documents):\n",
        "        # Tokenize the document\n",
        "        terms = tokenize(document)\n",
        "\n",
        "        # Count the term frequencies\n",
        "        term_freqs = {}\n",
        "        for term in terms:\n",
        "            term_freqs[term] = term_freqs.get(term, 0) + 1\n",
        "\n",
        "        # Normalize the term frequencies by the length of the document\n",
        "        doc_length = len(terms)\n",
        "        term_freqs = {term: freq / doc_length for term, freq in term_freqs.items()}\n",
        "        doc_term_freqs.append(term_freqs)\n",
        "\n",
        "        # Add the document to the index for each term it contains\n",
        "        for term in term_freqs:\n",
        "            if term not in index:\n",
        "                index[term] = []\n",
        "            index[term].append((i, term_freqs[term]))\n",
        "\n",
        "    # Calculate the inverse document frequencies\n",
        "    N = len(documents)\n",
        "    idfs = {term: math.log((N - len(postings) + 0.5) / (len(postings) + 0.5)) for term, postings in index.items()}\n",
        "\n",
        "    # Return the inverted index and document term frequencies\n",
        "    return index, doc_term_freqs, idfs"
      ],
      "metadata": {
        "id": "cvygbn4DuQda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index, doc_term_freqs, idfs = build_index(documents)"
      ],
      "metadata": {
        "id": "scmNO03PfapS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9DDFJEubrlND"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}